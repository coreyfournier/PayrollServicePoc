version: '3.8'

services:
  # Zookeeper for Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - payroll-network
    healthcheck:
      test: echo srvr | nc localhost 2181 || exit 1
      interval: 10s
      timeout: 5s
      retries: 5

  # Kafka message broker
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - payroll-network
    healthcheck:
      test: kafka-broker-api-versions --bootstrap-server localhost:9092 || exit 1
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s

  # Kafka topic initialization (standalone — normally handled by seed)
  kafka-init:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-init
    profiles: ["init"]
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint: ['/bin/sh', '-c']
    command: |
      "
      echo 'Creating Kafka topics...'
      kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic employee-events
      kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic timeentry-events
      kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic taxinfo-events
      kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic deduction-events
      kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic payperiod-hours-changed
      kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic employee-gross-pay
      kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic employee-net-pay --config cleanup.policy=compact,delete
      kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic employee-search --config cleanup.policy=compact
      kafka-topics --create --if-not-exists --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic employee-info --config cleanup.policy=compact

      echo 'Topics created successfully:'
      kafka-topics --list --bootstrap-server kafka:9092
      "
    networks:
      - payroll-network

  # MongoDB database (replica set for transaction support)
  mongodb:
    image: mongo:7.0
    container_name: mongodb
    command: ["--replSet", "rs0", "--bind_ip_all"]
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
    networks:
      - payroll-network
    healthcheck:
      test: |
        mongosh --eval '
          try {
            rs.status().ok
          } catch(e) {
            rs.initiate({_id: "rs0", members: [{_id: 0, host: "mongodb:27017"}]});
            sleep(1000);
            rs.status().ok
          }
        ' --quiet || exit 1
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 15s

  # MySQL database
  mysql:
    image: mysql:8.0
    container_name: mysql
    ports:
      - "3306:3306"
    environment:
      MYSQL_ROOT_PASSWORD: root_password
      MYSQL_DATABASE: listener_db
      MYSQL_USER: listener_user
      MYSQL_PASSWORD: listener_password
    volumes:
      - mysql_data:/var/lib/mysql
    networks:
      - payroll-network
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-proot_password"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Zipkin for distributed tracing
  zipkin:
    image: openzipkin/zipkin:latest
    container_name: zipkin
    ports:
      - "9411:9411"
    networks:
      - payroll-network

  # ksqlDB server for stream processing
  ksqldb-server:
    image: confluentinc/cp-ksqldb-server:7.7.1
    container_name: ksqldb-server
    ports:
      - "8088:8088"
    environment:
      KSQL_BOOTSTRAP_SERVERS: kafka:9092
      KSQL_LISTENERS: http://0.0.0.0:8088/
      KSQL_KSQL_SERVICE_ID: payroll_ksqldb_
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_NAME: KSQL_PROCESSING_LOG
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - payroll-network

  # ksqlDB init container — standalone (normally handled by seed)
  ksqldb-init:
    image: confluentinc/cp-ksqldb-server:7.7.1
    container_name: ksqldb-init
    profiles: ["init"]
    depends_on:
      - ksqldb-server
    entrypoint: /bin/bash
    command:
      - -c
      - |
        echo 'Waiting for ksqlDB server to be ready...'
        while ! curl -sf http://ksqldb-server:8088/info > /dev/null 2>&1; do
          echo 'ksqlDB not ready yet, retrying in 5s...'
          sleep 5
        done
        echo 'ksqlDB is ready.'

        # Terminate all running queries so DROP statements can succeed
        echo 'Terminating existing queries...'
        QUERY_IDS=$$(curl -sf http://ksqldb-server:8088/ksql \
          -H 'Content-Type: application/vnd.ksql.v1+json' \
          -d '{"ksql": "SHOW QUERIES;"}' \
          | grep -o '"id":"[^"]*"' | sed 's/"id":"//;s/"//')
        for qid in $$QUERY_IDS; do
          echo "Terminating query $$qid"
          curl -sf -X POST http://ksqldb-server:8088/ksql \
            -H 'Content-Type: application/vnd.ksql.v1+json' \
            -d "{\"ksql\": \"TERMINATE $${qid};\"}" || true
          sleep 1
        done

        echo 'Submitting SQL statements...'

        # Read the SQL file and split on semicolons, submitting each statement
        while IFS= read -r stmt; do
          # Skip empty lines and comments
          [ -z "$$stmt" ] && continue

          echo "Executing: $$stmt"
          curl -sf -X POST http://ksqldb-server:8088/ksql \
            -H 'Content-Type: application/vnd.ksql.v1+json' \
            -d "{\"ksql\": \"$${stmt}\", \"streamsProperties\": {}}" \
            && echo -e '\nStatement executed successfully.' \
            || echo -e '\nStatement execution failed.'
          sleep 2
        done < <(
          # Collapse multi-line SQL into single-line statements split by semicolons
          sed 's/--.*$$//' /statements.sql | tr '\n' ' ' | sed 's/;/;\n/g' | sed 's/^[[:space:]]*//' | grep -v '^$$'
        )

        echo 'All ksqlDB statements submitted.'
    volumes:
      - ./ksqldb/statements.sql:/statements.sql
    networks:
      - payroll-network

  # Net Pay Processor — Kafka Streams app computing net pay from gross pay, taxes, and deductions
  net-pay-processor:
    build:
      context: ./src/NetPayProcessor
      dockerfile: Dockerfile
    container_name: net-pay-processor
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      APPLICATION_ID: net-pay-processor
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - payroll-network
    restart: on-failure

  # Elasticsearch for search
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: elasticsearch
    ports:
      - "9200:9200"
    environment:
      discovery.type: single-node
      xpack.security.enabled: "false"
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - payroll-network
    healthcheck:
      test: curl -sf http://localhost:9200/_cluster/health || exit 1
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  # Kafka Connect with Elasticsearch Sink Connector
  kafka-connect:
    build:
      context: ./docker
      dockerfile: Dockerfile.kafka-connect
    container_name: kafka-connect
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: payroll-connect
      CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components
    depends_on:
      kafka:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    networks:
      - payroll-network
    healthcheck:
      test: curl -sf http://localhost:8083/connectors || exit 1
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  # Elasticsearch Updater — combines employee-info + employee-net-pay → employee-search topic
  elasticsearch-updater:
    build:
      context: ./src/ElasticsearchUpdater
      dockerfile: Dockerfile
    container_name: elasticsearch-updater
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      APPLICATION_ID: elasticsearch-updater
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - payroll-network
    restart: on-failure

  # Kafka UI for monitoring
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: payroll-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
      KAFKA_CLUSTERS_0_KSQLDBSERVER: http://ksqldb-server:8088
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: payroll-connect
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: http://kafka-connect:8083
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - payroll-network

  # Payroll API Service
  payroll-api:
    build:
      context: .
      dockerfile: docker/Dockerfile
    container_name: payroll-api
    ports:
      - "5000:80"
    environment:
      - ASPNETCORE_ENVIRONMENT=Development
      - ASPNETCORE_URLS=http://+:80
      - MongoDB__ConnectionString=mongodb://mongodb:27017/?replicaSet=rs0
      - MongoDB__DatabaseName=payroll_db
      - Features__UseDaprOutbox=true
      # Dapr sidecar endpoint configuration
      - DAPR_HTTP_ENDPOINT=http://payroll-api-dapr:3500
      - DAPR_GRPC_ENDPOINT=http://payroll-api-dapr:50001
    depends_on:
      mongodb:
        condition: service_healthy
      kafka:
        condition: service_healthy
    networks:
      - payroll-network
    healthcheck:
      test: curl -sf http://localhost:80/api/employees || exit 1
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 20s

  # Dapr sidecar for Payroll API
  payroll-api-dapr:
    image: "daprio/daprd:1.16.0"
    container_name: payroll-api-dapr
    command: [
      "./daprd",
      "-app-id", "payroll-api",
      "-app-port", "80",
      "-app-channel-address", "payroll-api",
      "-dapr-http-port", "3500",
      "-dapr-grpc-port", "50001",
      "-resources-path", "/components",
      "-config", "/config/config.yaml"
    ]
    ports:
      - "3500:3500"   # Dapr HTTP port
      - "50001:50001" # Dapr gRPC port
    volumes:
      - ./dapr/components:/components
      - ./dapr:/config
    depends_on:
      payroll-api:
        condition: service_healthy
      kafka:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    networks:
      - payroll-network
    restart: on-failure

  # Frontend Application
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: payroll-frontend
    ports:
      - "3000:80"
    depends_on:
      payroll-api:
        condition: service_healthy
    networks:
      - payroll-network

  # ListenerApi Service
  listener-api:
    build:
      context: .
      dockerfile: docker/Dockerfile.listenerapi
    container_name: listener-api
    ports:
      - "5001:80"
    environment:
      - ASPNETCORE_ENVIRONMENT=Development
      - ASPNETCORE_URLS=http://+:80
      - ConnectionStrings__DefaultConnection=Server=mysql;Database=listener_db;User=listener_user;Password=listener_password;
      # Dapr sidecar endpoint configuration
      - DAPR_HTTP_ENDPOINT=http://listener-api-dapr:3501
      - DAPR_GRPC_ENDPOINT=http://listener-api-dapr:50002
    depends_on:
      mysql:
        condition: service_healthy
      kafka:
        condition: service_healthy
    networks:
      - payroll-network
    healthcheck:
      test: curl -sf http://localhost:80/graphql || exit 1
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s

  # Dapr sidecar for ListenerApi
  listener-api-dapr:
    image: "daprio/daprd:1.16.0"
    container_name: listener-api-dapr
    command: [
      "./daprd",
      "-app-id", "listener-api",
      "-app-port", "80",
      "-app-channel-address", "listener-api",
      "-dapr-http-port", "3501",
      "-dapr-grpc-port", "50002",
      "-resources-path", "/components",
      "-config", "/config/config.yaml"
    ]
    volumes:
      - ./dapr/components:/components
      - ./dapr:/config
    depends_on:
      listener-api:
        condition: service_healthy
      kafka:
        condition: service_healthy
    networks:
      - payroll-network
    restart: on-failure

  # ListenerClient Frontend
  listener-client:
    build:
      context: ./listenerClient
      dockerfile: Dockerfile
    container_name: listener-client
    ports:
      - "3001:80"
    depends_on:
      listener-api:
        condition: service_healthy
    networks:
      - payroll-network

  # Seed script — creates topics, initializes ksqlDB, seeds data, registers ES connector
  seed:
    image: confluentinc/cp-kafka:7.5.0
    container_name: seed
    depends_on:
      kafka:
        condition: service_healthy
      payroll-api:
        condition: service_healthy
      listener-api:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
      kafka-connect:
        condition: service_healthy
    volumes:
      - ./scripts/seed.sh:/seed.sh
      - ./ksqldb/statements.sql:/statements.sql
    entrypoint: ["bash", "/seed.sh"]
    networks:
      - payroll-network

networks:
  payroll-network:
    driver: bridge

volumes:
  mongodb_data:
  mysql_data:
  elasticsearch_data:
